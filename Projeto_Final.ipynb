{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "ds = pd.read_csv('TRNcod.xls', delimiter = \"\\t\")\n",
    "\n",
    "# Shuffle no dataset\n",
    "ds = ds.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([d for d in ds.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanhos de inadimplentes: \n",
      "Treino: 127549\n",
      "Teste: 33524\n",
      "Validação: 63775\n",
      "\n",
      "Tamanhos de adimplentes: \n",
      "Treino: 127549\n",
      "Teste: 63774\n",
      "Validação: 63775\n",
      "\n",
      "Colunas: 246\n"
     ]
    }
   ],
   "source": [
    "# inadimplentes = pd.DataFrame(list(filter(lambda x: x == 1, ds['IND_BOM_1_2'])))\n",
    "\n",
    "# Selecionando quem é inadimplente\n",
    "inadimplentes = ds[ds['IND_BOM_1_2'] == 1]\n",
    "\n",
    "# Selecionando quem é adimplente\n",
    "adimplente = ds[ds['IND_BOM_1_2'] == 0]\n",
    "\n",
    "global treino_ina\n",
    "global teste_ina\n",
    "global valid_ina\n",
    "global treino_adi\n",
    "global teste_adi\n",
    "global valid_adi\n",
    "\n",
    "\n",
    "# Dividindo datasets\n",
    "treino_ina = inadimplentes[:int(len(inadimplentes)/2)]\n",
    "teste_ina  = inadimplentes[int(len(inadimplentes)/2):int((len(inadimplentes)*3)/4)]\n",
    "valid_ina  = inadimplentes[int((len(inadimplentes)*3)/4):]\n",
    "\n",
    "treino_adi = adimplente[:int(len(adimplente)/2)]\n",
    "teste_adi  = adimplente[int(len(adimplente)/2):int((len(adimplente)*3)/4)]\n",
    "valid_adi  = adimplente[int((len(adimplente)*3)/4):]\n",
    "\n",
    "# Equalizando tamanho de datasets treino e validação dos inadimplentes ao de adimplentes\n",
    "treino_ina = treino_ina.loc[treino_ina.index.repeat(2)].drop('INDEX', axis=1)\n",
    "treino_ina[\"COPIA\"] = treino_ina.duplicated()\n",
    "treino_ina.sort_values(by=\"COPIA\", inplace=True, ignore_index=True)\n",
    "treino_ina = treino_ina.iloc[ : ( len(treino_adi) - len(treino_ina) ), :  ]\n",
    "treino_ina.drop(columns=[\"COPIA\"], axis=1)\n",
    "\n",
    "valid_ina  = valid_ina.loc[valid_ina.index.repeat(2)].drop('INDEX', axis=1)\n",
    "valid_ina[\"COPIA\"] = valid_ina.duplicated()\n",
    "valid_ina.sort_values(by=\"COPIA\", inplace=True, ignore_index=True)\n",
    "valid_ina = valid_ina.iloc[ : ( len(valid_adi) - len(valid_ina) ), :  ]\n",
    "valid_ina.drop(columns=[\"COPIA\"], axis=1)\n",
    "# Fim da equalização\n",
    "\n",
    "print('Tamanhos de inadimplentes: \\nTreino: {}\\nTeste: {}\\nValidação: {}\\n'.format(len(treino_ina.values), len(teste_ina.values), len(valid_ina.values)))\n",
    "print('Tamanhos de adimplentes: \\nTreino: {}\\nTeste: {}\\nValidação: {}\\n'.format(len(treino_adi.values), len(teste_adi.values), len(valid_adi.values)))\n",
    "print('Colunas: {}'.format(len([d for d in ds.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "\n",
    "# # Create correlation matrix\n",
    "# corr_matrix = ds.corr().abs()\n",
    "\n",
    "# # Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# # Find features with correlation greater than 0.89\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.89)]\n",
    "\n",
    "# # Drop features \n",
    "# # ds.drop(to_drop, axis=1, inplace=True)\n",
    "# ds.columns\n",
    "\n",
    "\n",
    "# Colunas mais correlacionadas\n",
    "# l = []\n",
    "# for c in ds.columns[:-1]:\n",
    "#     l.append({c: ds[c].corr(ds['IND_BOM_1_1'])})\n",
    "#     # if (ds[c].corr(ds['IND_BOM_1_1']) > 0.0):\n",
    "#         # print( c, ds[c].corr(ds['IND_BOM_1_1']) )\n",
    "# l.sort(key=lambda x: list(x.values())[0])\n",
    "# print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passos a fazer\n",
    "# Esquema de experimentação (passo a passo do que vamos testar)\n",
    "# MLP & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "938/938 [==============================] - 2s 1ms/step - loss: 5.1866 - accuracy: 0.6314\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.2277 - accuracy: 0.6375\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.2522 - accuracy: 0.6424\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.1888 - accuracy: 0.6379\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.1775 - accuracy: 0.6308\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.1360 - accuracy: 0.6177\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.1588 - accuracy: 0.6255\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 1s 991us/step - loss: 5.2004 - accuracy: 0.6336\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.2032 - accuracy: 0.6401\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.1713 - accuracy: 0.6298\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 5.1918 - accuracy: 0.6308\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 1s 979us/step - loss: 5.1987 - accuracy: 0.6321\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 1s 927us/step - loss: 5.1920 - accuracy: 0.6313\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# MLP\n",
    "model = Sequential()\n",
    "\n",
    "# Dropout\n",
    "layer = Dropout(randint(0,100)/100)\n",
    "\n",
    "# Regularização\n",
    "# ???\n",
    "\n",
    "# ds_2 = ds\n",
    "ds_2 = ds.iloc[ 0:30000, : ]\n",
    "\n",
    "# Taxa de parada quando não mais evoluir\n",
    "\n",
    "model.add(Dense(512,input_dim=245, activation='tanh'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "trY = ds_2['IND_BOM_1_2']\n",
    "trX = ds_2.drop('IND_BOM_1_2', axis=1)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "\n",
    "# Parada antecipada caso em 10 epochs ela deixe de melhorar\n",
    "paradinha = EarlyStopping(monitor='accuracy', mode='max', patience=10)\n",
    "\n",
    "history = model.fit(trX, trY, epochs=50, verbose=1, callbacks=[paradinha])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 1s 508us/step - loss: 5.2510 - accuracy: 0.6596\n"
     ]
    }
   ],
   "source": [
    "ds_3 = ds.iloc[ 50001:100001, :]\n",
    "trY = ds_3['IND_BOM_1_2']\n",
    "trX = ds_3.drop('IND_BOM_1_2', axis=1)\n",
    "x, y = model.evaluate(trX, trY, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.888 (0.081)\n"
     ]
    }
   ],
   "source": [
    "# Cuidado: usar esses parâmetros muito elevados, ou o default (100 estimadores e profundidade ilimitada) vai travar seu computador\n",
    "\n",
    "# Parâmetros default:\n",
    "# n_estimators=100, *,\n",
    "# criterion=\"gini\",\n",
    "# max_depth=None,\n",
    "# min_samples_split=2,\n",
    "# min_samples_leaf=1,\n",
    "# min_weight_fraction_leaf=0.,\n",
    "# max_features=\"auto\",\n",
    "# max_leaf_nodes=None,\n",
    "# min_impurity_decrease=0.,\n",
    "# min_impurity_split=None,\n",
    "# bootstrap=True,\n",
    "# oob_score=False,\n",
    "# n_jobs=None,\n",
    "# random_state=None,\n",
    "# verbose=0,\n",
    "# warm_start=False,\n",
    "# class_weight=None,\n",
    "# ccp_alpha=0.0,\n",
    "# max_samples=None\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=5, max_depth=8)\n",
    "n_scores = cross_val_score(random_forest, ds.drop('IND_BOM_1_2', axis=1), ds['IND_BOM_1_2'], scoring='accuracy', n_jobs=1, error_score='raise')\n",
    "\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros default:\n",
    "# *,loss='deviance', \n",
    "# learning_rate=0.1, \n",
    "# n_estimators=100,\n",
    "# subsample=1.0, \n",
    "# criterion='friedman_mse', \n",
    "# min_samples_split=2,\n",
    "# min_samples_leaf=1, \n",
    "# min_weight_fraction_leaf=0.,\n",
    "# max_depth=3, \n",
    "# min_impurity_decrease=0.,\n",
    "# min_impurity_split=None, \n",
    "# random_state=None, \n",
    "# max_features=None, verbose=0,\n",
    "# max_leaf_nodes=None, \n",
    "# warm_start=False,\n",
    "# validation_fraction=0.1, \n",
    "# n_iter_no_change=None, \n",
    "# tol=1e-4,\n",
    "# ccp_alpha=0.0\n",
    "\n",
    "gradient_boost = GradientBoostingClassifier(n_estimators = 5, max_depth = 8)\n",
    "n_scores = cross_val_score(gradient_boost, ds.drop('IND_BOM_1_2', axis=1), ds['IND_BOM_1_2'], scoring='accuracy', n_jobs=1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
