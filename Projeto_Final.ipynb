{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint\n",
    "from tensorflow import keras \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "ds = pd.read_csv('TRNcod.xls', delimiter = \"\\t\")\n",
    "\n",
    "# Shuffle no dataset\n",
    "ds = ds.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([d for d in ds.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitor/.local/lib/python3.8/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanhos de inadimplentes: \n",
      "Treino: 127549\n",
      "Teste: 33524\n",
      "Validação: 63775\n",
      "\n",
      "Tamanhos de adimplentes: \n",
      "Treino: 127549\n",
      "Teste: 63774\n",
      "Validação: 63775\n",
      "\n",
      "Colunas: 246\n"
     ]
    }
   ],
   "source": [
    "# inadimplentes = pd.DataFrame(list(filter(lambda x: x == 1, ds['IND_BOM_1_2'])))\n",
    "\n",
    "# Selecionando quem é inadimplente\n",
    "inadimplentes = ds[ds['IND_BOM_1_2'] == 1]\n",
    "\n",
    "# Selecionando quem é adimplente\n",
    "adimplente = ds[ds['IND_BOM_1_2'] == 0]\n",
    "\n",
    "global treino_ina\n",
    "global teste_ina\n",
    "global valid_ina\n",
    "global treino_adi\n",
    "global teste_adi\n",
    "global valid_adi\n",
    "\n",
    "\n",
    "# Dividindo datasets\n",
    "treino_ina = inadimplentes[:int(len(inadimplentes)/2)]\n",
    "teste_ina  = inadimplentes[int(len(inadimplentes)/2):int((len(inadimplentes)*3)/4)]\n",
    "valid_ina  = inadimplentes[int((len(inadimplentes)*3)/4):]\n",
    "\n",
    "treino_adi = adimplente[:int(len(adimplente)/2)]\n",
    "teste_adi  = adimplente[int(len(adimplente)/2):int((len(adimplente)*3)/4)]\n",
    "valid_adi  = adimplente[int((len(adimplente)*3)/4):]\n",
    "\n",
    "# Equalizando tamanho de datasets treino e validação dos inadimplentes ao de adimplentes\n",
    "treino_ina = treino_ina.loc[treino_ina.index.repeat(2)].drop('INDEX', axis=1)\n",
    "treino_ina[\"COPIA\"] = treino_ina.duplicated()\n",
    "treino_ina.sort_values(by=\"COPIA\", inplace=True, ignore_index=True)\n",
    "treino_ina = treino_ina.iloc[ : ( len(treino_adi) - len(treino_ina) ), :  ]\n",
    "treino_ina.drop(columns=[\"COPIA\"], axis=1, inplace=True)\n",
    "\n",
    "treino_adi.drop(columns=['INDEX'], axis=1, inplace=True)\n",
    "\n",
    "valid_ina  = valid_ina.loc[valid_ina.index.repeat(2)].drop('INDEX', axis=1)\n",
    "valid_ina[\"COPIA\"] = valid_ina.duplicated()\n",
    "valid_ina.sort_values(by=\"COPIA\", inplace=True, ignore_index=True)\n",
    "valid_ina = valid_ina.iloc[ : ( len(valid_adi) - len(valid_ina) ), :  ]\n",
    "valid_ina.drop(columns=[\"COPIA\"], axis=1, inplace=True)\n",
    "# Fim da equalização\n",
    "\n",
    "print('Tamanhos de inadimplentes: \\nTreino: {}\\nTeste: {}\\nValidação: {}\\n'.format(len(treino_ina.values), len(teste_ina.values), len(valid_ina.values)))\n",
    "print('Tamanhos de adimplentes: \\nTreino: {}\\nTeste: {}\\nValidação: {}\\n'.format(len(treino_adi.values), len(teste_adi.values), len(valid_adi.values)))\n",
    "print('Colunas: {}'.format(len([d for d in ds.columns])))\n",
    "ds.drop(labels=\"INDEX\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "\n",
    "# # Create correlation matrix\n",
    "# corr_matrix = ds.corr().abs()\n",
    "\n",
    "# # Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# # Find features with correlation greater than 0.89\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.89)]\n",
    "\n",
    "# # Drop features \n",
    "# # ds.drop(to_drop, axis=1, inplace=True)\n",
    "# ds.columns\n",
    "\n",
    "\n",
    "# Colunas mais correlacionadas\n",
    "# l = []\n",
    "# for c in ds.columns[:-1]:\n",
    "#     l.append({c: ds[c].corr(ds['IND_BOM_1_1'])})\n",
    "#     # if (ds[c].corr(ds['IND_BOM_1_1']) > 0.0):\n",
    "#         # print( c, ds[c].corr(ds['IND_BOM_1_1']) )\n",
    "# l.sort(key=lambda x: list(x.values())[0])\n",
    "# print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passos a fazer\n",
    "# Esquema de experimentação (passo a passo do que vamos testar)\n",
    "# MLP & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "12163/12163 [==============================] - 9s 593us/step - loss: 0.6022 - accuracy: 0.6707\n",
      "Epoch 2/10000\n",
      "12163/12163 [==============================] - 7s 577us/step - loss: 0.5943 - accuracy: 0.6772\n",
      "Epoch 3/10000\n",
      "12163/12163 [==============================] - 7s 572us/step - loss: 0.5921 - accuracy: 0.6796\n",
      "Epoch 4/10000\n",
      "12163/12163 [==============================] - 7s 574us/step - loss: 0.5909 - accuracy: 0.6807\n",
      "Epoch 5/10000\n",
      "12163/12163 [==============================] - 7s 590us/step - loss: 0.5901 - accuracy: 0.6815\n",
      "Epoch 6/10000\n",
      "12163/12163 [==============================] - 7s 594us/step - loss: 0.5893 - accuracy: 0.6822\n",
      "Epoch 7/10000\n",
      "12163/12163 [==============================] - 8s 641us/step - loss: 0.5888 - accuracy: 0.6823\n",
      "Epoch 8/10000\n",
      "12163/12163 [==============================] - 7s 607us/step - loss: 0.5883 - accuracy: 0.6829\n",
      "Epoch 9/10000\n",
      "12163/12163 [==============================] - 7s 554us/step - loss: 0.5881 - accuracy: 0.6827\n",
      "Epoch 10/10000\n",
      "12163/12163 [==============================] - 6s 532us/step - loss: 0.5875 - accuracy: 0.6836\n",
      "Epoch 11/10000\n",
      "12163/12163 [==============================] - 6s 531us/step - loss: 0.5870 - accuracy: 0.6838\n",
      "Epoch 12/10000\n",
      "12163/12163 [==============================] - 7s 580us/step - loss: 0.5868 - accuracy: 0.6840\n",
      "Epoch 13/10000\n",
      "12163/12163 [==============================] - 7s 579us/step - loss: 0.5866 - accuracy: 0.6843\n",
      "Epoch 14/10000\n",
      "12163/12163 [==============================] - 7s 578us/step - loss: 0.5860 - accuracy: 0.6847\n",
      "Epoch 15/10000\n",
      "12163/12163 [==============================] - 7s 597us/step - loss: 0.5859 - accuracy: 0.6853\n",
      "Epoch 16/10000\n",
      "12163/12163 [==============================] - 7s 579us/step - loss: 0.5857 - accuracy: 0.6849\n",
      "Epoch 17/10000\n",
      "12163/12163 [==============================] - 7s 579us/step - loss: 0.5850 - accuracy: 0.6853\n",
      "Epoch 18/10000\n",
      "12163/12163 [==============================] - 7s 594us/step - loss: 0.5849 - accuracy: 0.6862\n",
      "Epoch 19/10000\n",
      "12163/12163 [==============================] - 7s 592us/step - loss: 0.5847 - accuracy: 0.6856\n",
      "Epoch 20/10000\n",
      "12163/12163 [==============================] - 7s 584us/step - loss: 0.5844 - accuracy: 0.6866\n",
      "Epoch 21/10000\n",
      "12163/12163 [==============================] - 7s 587us/step - loss: 0.5841 - accuracy: 0.6860\n",
      "Epoch 22/10000\n",
      "12163/12163 [==============================] - 7s 589us/step - loss: 0.5841 - accuracy: 0.6864\n",
      "Epoch 23/10000\n",
      "12163/12163 [==============================] - 7s 569us/step - loss: 0.5839 - accuracy: 0.6866\n",
      "Epoch 24/10000\n",
      "12163/12163 [==============================] - 7s 594us/step - loss: 0.5836 - accuracy: 0.6870\n",
      "Epoch 25/10000\n",
      "12163/12163 [==============================] - 7s 579us/step - loss: 0.5834 - accuracy: 0.6872\n",
      "Epoch 26/10000\n",
      "12163/12163 [==============================] - 7s 575us/step - loss: 0.5835 - accuracy: 0.6870\n",
      "Epoch 27/10000\n",
      "12163/12163 [==============================] - 7s 591us/step - loss: 0.5834 - accuracy: 0.6871\n",
      "Epoch 28/10000\n",
      "12163/12163 [==============================] - 7s 588us/step - loss: 0.5830 - accuracy: 0.6873\n",
      "Epoch 29/10000\n",
      "12163/12163 [==============================] - 7s 582us/step - loss: 0.5831 - accuracy: 0.6875\n",
      "Epoch 30/10000\n",
      "12163/12163 [==============================] - 7s 592us/step - loss: 0.5829 - accuracy: 0.6875\n",
      "Epoch 31/10000\n",
      "12163/12163 [==============================] - 7s 582us/step - loss: 0.5828 - accuracy: 0.6872\n",
      "Epoch 32/10000\n",
      "12163/12163 [==============================] - 7s 593us/step - loss: 0.5824 - accuracy: 0.6881\n",
      "Epoch 33/10000\n",
      "12163/12163 [==============================] - 7s 581us/step - loss: 0.5822 - accuracy: 0.6880\n",
      "Epoch 34/10000\n",
      "12163/12163 [==============================] - 7s 602us/step - loss: 0.5822 - accuracy: 0.6883\n",
      "Epoch 35/10000\n",
      "12163/12163 [==============================] - 8s 619us/step - loss: 0.5821 - accuracy: 0.6881\n",
      "Epoch 36/10000\n",
      "12163/12163 [==============================] - 7s 613us/step - loss: 0.5822 - accuracy: 0.6888\n",
      "Epoch 37/10000\n",
      "12163/12163 [==============================] - 7s 599us/step - loss: 0.5821 - accuracy: 0.6888\n",
      "Epoch 38/10000\n",
      "12163/12163 [==============================] - 7s 593us/step - loss: 0.5820 - accuracy: 0.6880\n",
      "Epoch 39/10000\n",
      "12163/12163 [==============================] - 7s 543us/step - loss: 0.5817 - accuracy: 0.6890\n",
      "Epoch 40/10000\n",
      "12163/12163 [==============================] - 6s 520us/step - loss: 0.5816 - accuracy: 0.6888\n",
      "Epoch 41/10000\n",
      "12163/12163 [==============================] - 6s 499us/step - loss: 0.5817 - accuracy: 0.6883\n",
      "Epoch 42/10000\n",
      "12163/12163 [==============================] - 6s 505us/step - loss: 0.5816 - accuracy: 0.6888\n",
      "Epoch 43/10000\n",
      "12163/12163 [==============================] - 6s 498us/step - loss: 0.5816 - accuracy: 0.6884\n",
      "Epoch 44/10000\n",
      "12163/12163 [==============================] - 6s 501us/step - loss: 0.5816 - accuracy: 0.6886\n",
      "Epoch 45/10000\n",
      "12163/12163 [==============================] - 6s 497us/step - loss: 0.5813 - accuracy: 0.6890\n",
      "Epoch 46/10000\n",
      "12163/12163 [==============================] - 6s 501us/step - loss: 0.5814 - accuracy: 0.6884\n",
      "Epoch 47/10000\n",
      "12163/12163 [==============================] - 6s 505us/step - loss: 0.5814 - accuracy: 0.6890\n",
      "Epoch 48/10000\n",
      "12163/12163 [==============================] - 7s 611us/step - loss: 0.5814 - accuracy: 0.6885\n",
      "Epoch 49/10000\n",
      "12163/12163 [==============================] - 7s 578us/step - loss: 0.5810 - accuracy: 0.6889\n",
      "Epoch 50/10000\n",
      "12163/12163 [==============================] - 7s 548us/step - loss: 0.5812 - accuracy: 0.6893\n",
      "Epoch 51/10000\n",
      "12163/12163 [==============================] - 7s 595us/step - loss: 0.5811 - accuracy: 0.6890\n",
      "Epoch 52/10000\n",
      "12163/12163 [==============================] - 7s 578us/step - loss: 0.5812 - accuracy: 0.6894\n",
      "Epoch 53/10000\n",
      "12163/12163 [==============================] - 7s 583us/step - loss: 0.5809 - accuracy: 0.6891\n",
      "Epoch 54/10000\n",
      "12163/12163 [==============================] - 7s 580us/step - loss: 0.5807 - accuracy: 0.6894\n",
      "Epoch 55/10000\n",
      "12163/12163 [==============================] - 7s 591us/step - loss: 0.5806 - accuracy: 0.6891\n",
      "Epoch 56/10000\n",
      "12163/12163 [==============================] - 7s 598us/step - loss: 0.5806 - accuracy: 0.6895\n",
      "Epoch 57/10000\n",
      "12163/12163 [==============================] - 7s 597us/step - loss: 0.5808 - accuracy: 0.6896\n",
      "Epoch 58/10000\n",
      "12163/12163 [==============================] - 7s 585us/step - loss: 0.5806 - accuracy: 0.6896\n",
      "Epoch 59/10000\n",
      "12163/12163 [==============================] - 7s 588us/step - loss: 0.5806 - accuracy: 0.6894\n",
      "Epoch 60/10000\n",
      "12163/12163 [==============================] - 7s 598us/step - loss: 0.5805 - accuracy: 0.6898\n",
      "Epoch 61/10000\n",
      "12163/12163 [==============================] - 7s 586us/step - loss: 0.5806 - accuracy: 0.6894\n",
      "Epoch 62/10000\n",
      "12163/12163 [==============================] - 7s 606us/step - loss: 0.5805 - accuracy: 0.6893\n",
      "Epoch 63/10000\n",
      "12163/12163 [==============================] - 7s 616us/step - loss: 0.5804 - accuracy: 0.6896\n",
      "Epoch 64/10000\n",
      "12163/12163 [==============================] - 7s 604us/step - loss: 0.5804 - accuracy: 0.6889\n",
      "Epoch 65/10000\n",
      "12163/12163 [==============================] - 7s 597us/step - loss: 0.5805 - accuracy: 0.6903\n",
      "Epoch 66/10000\n",
      "12163/12163 [==============================] - 7s 581us/step - loss: 0.5802 - accuracy: 0.6899\n",
      "Epoch 67/10000\n",
      "12163/12163 [==============================] - 7s 588us/step - loss: 0.5801 - accuracy: 0.6898\n",
      "Epoch 68/10000\n",
      "12163/12163 [==============================] - 7s 591us/step - loss: 0.5805 - accuracy: 0.6896\n",
      "Epoch 69/10000\n",
      "12163/12163 [==============================] - 7s 573us/step - loss: 0.5801 - accuracy: 0.6899\n",
      "Epoch 70/10000\n",
      "12163/12163 [==============================] - 7s 596us/step - loss: 0.5803 - accuracy: 0.6892\n",
      "Epoch 71/10000\n",
      "12163/12163 [==============================] - 7s 584us/step - loss: 0.5799 - accuracy: 0.6901\n",
      "Epoch 72/10000\n",
      "12163/12163 [==============================] - 7s 586us/step - loss: 0.5800 - accuracy: 0.6894\n",
      "Epoch 73/10000\n",
      "12163/12163 [==============================] - 7s 589us/step - loss: 0.5799 - accuracy: 0.6902\n",
      "Epoch 74/10000\n",
      "12163/12163 [==============================] - 7s 581us/step - loss: 0.5797 - accuracy: 0.6899\n",
      "Epoch 75/10000\n",
      "12163/12163 [==============================] - 7s 595us/step - loss: 0.5799 - accuracy: 0.6902\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "\n",
    "# MLP\n",
    "model = Sequential()\n",
    "\n",
    "# Dropout\n",
    "layer = Dropout(randint(0,100)/100)\n",
    "\n",
    "# Regularização\n",
    "# ???\n",
    "\n",
    "ds_2 = ds\n",
    "# ds_2 = ds.iloc[ 0:300, : ]\n",
    "\n",
    "trY = ds_2['IND_BOM_1_2']\n",
    "trX = ds_2.drop('IND_BOM_1_2', axis=1)\n",
    "trX = trX.drop('IND_BOM_1_1', axis=1)\n",
    "# trX = trX.drop(columns=['INDEX'], axis=1)\n",
    "\n",
    "# Taxa de parada quando não mais evoluir\n",
    "\n",
    "model.add(Dense(32,input_dim=243, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "\n",
    "# Parada antecipada caso em 10 epochs ela deixe de melhorar\n",
    "paradinha = EarlyStopping(monitor='accuracy', mode='max', patience=10)\n",
    "\n",
    "history = model.fit(trX, trY, epochs=10000, verbose=1, callbacks=[paradinha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "313/313 [==============================] - 1s 595us/step - loss: 0.6398 - accuracy: 0.6446\n",
      "Epoch 2/10000\n",
      "313/313 [==============================] - 0s 664us/step - loss: 0.6197 - accuracy: 0.6564\n",
      "Epoch 3/10000\n",
      "313/313 [==============================] - 0s 650us/step - loss: 0.6125 - accuracy: 0.6602\n",
      "Epoch 4/10000\n",
      "313/313 [==============================] - 0s 641us/step - loss: 0.6058 - accuracy: 0.6694\n",
      "Epoch 5/10000\n",
      "313/313 [==============================] - 0s 732us/step - loss: 0.6016 - accuracy: 0.6727\n",
      "Epoch 6/10000\n",
      "313/313 [==============================] - 0s 766us/step - loss: 0.5974 - accuracy: 0.6789\n",
      "Epoch 7/10000\n",
      "313/313 [==============================] - 0s 613us/step - loss: 0.5935 - accuracy: 0.6800\n",
      "Epoch 8/10000\n",
      "313/313 [==============================] - 0s 860us/step - loss: 0.5882 - accuracy: 0.6809\n",
      "Epoch 9/10000\n",
      "313/313 [==============================] - 0s 666us/step - loss: 0.5878 - accuracy: 0.6817\n",
      "Epoch 10/10000\n",
      "313/313 [==============================] - 0s 651us/step - loss: 0.5815 - accuracy: 0.6898\n",
      "Epoch 11/10000\n",
      "313/313 [==============================] - 0s 590us/step - loss: 0.5802 - accuracy: 0.6903\n",
      "Epoch 12/10000\n",
      "313/313 [==============================] - 0s 615us/step - loss: 0.5749 - accuracy: 0.6943\n",
      "Epoch 13/10000\n",
      "313/313 [==============================] - 0s 613us/step - loss: 0.5733 - accuracy: 0.6962\n",
      "Epoch 14/10000\n",
      "313/313 [==============================] - 0s 622us/step - loss: 0.5682 - accuracy: 0.7021\n",
      "Epoch 15/10000\n",
      "313/313 [==============================] - 0s 619us/step - loss: 0.5642 - accuracy: 0.7058\n",
      "Epoch 16/10000\n",
      "313/313 [==============================] - 0s 558us/step - loss: 0.5627 - accuracy: 0.7053\n",
      "Epoch 17/10000\n",
      "313/313 [==============================] - 0s 584us/step - loss: 0.5582 - accuracy: 0.7104\n",
      "Epoch 18/10000\n",
      "313/313 [==============================] - 0s 605us/step - loss: 0.5550 - accuracy: 0.7140\n",
      "Epoch 19/10000\n",
      "313/313 [==============================] - 0s 595us/step - loss: 0.5516 - accuracy: 0.7158\n",
      "Epoch 20/10000\n",
      "313/313 [==============================] - 0s 538us/step - loss: 0.5481 - accuracy: 0.7160\n",
      "Epoch 21/10000\n",
      "313/313 [==============================] - 0s 585us/step - loss: 0.5464 - accuracy: 0.7216\n",
      "Epoch 22/10000\n",
      "313/313 [==============================] - 0s 553us/step - loss: 0.5405 - accuracy: 0.7227\n",
      "Epoch 23/10000\n",
      "313/313 [==============================] - 0s 604us/step - loss: 0.5405 - accuracy: 0.7179\n",
      "Epoch 24/10000\n",
      "313/313 [==============================] - 0s 599us/step - loss: 0.5360 - accuracy: 0.7285\n",
      "Epoch 25/10000\n",
      "313/313 [==============================] - 0s 565us/step - loss: 0.5307 - accuracy: 0.7305\n",
      "Epoch 26/10000\n",
      "313/313 [==============================] - 0s 604us/step - loss: 0.5302 - accuracy: 0.7318\n",
      "Epoch 27/10000\n",
      "313/313 [==============================] - 0s 591us/step - loss: 0.5261 - accuracy: 0.7361\n",
      "Epoch 28/10000\n",
      "313/313 [==============================] - 0s 773us/step - loss: 0.5230 - accuracy: 0.7352\n",
      "Epoch 29/10000\n",
      "313/313 [==============================] - 0s 571us/step - loss: 0.5207 - accuracy: 0.7417\n",
      "Epoch 30/10000\n",
      "313/313 [==============================] - 0s 615us/step - loss: 0.5186 - accuracy: 0.7376\n",
      "Epoch 31/10000\n",
      "313/313 [==============================] - 0s 607us/step - loss: 0.5150 - accuracy: 0.7426\n",
      "Epoch 32/10000\n",
      "313/313 [==============================] - 0s 579us/step - loss: 0.5099 - accuracy: 0.7440\n",
      "Epoch 33/10000\n",
      "313/313 [==============================] - 0s 550us/step - loss: 0.5082 - accuracy: 0.7466\n",
      "Epoch 34/10000\n",
      "313/313 [==============================] - 0s 573us/step - loss: 0.5072 - accuracy: 0.7462\n",
      "Epoch 35/10000\n",
      "313/313 [==============================] - 0s 613us/step - loss: 0.5037 - accuracy: 0.7499\n",
      "Epoch 36/10000\n",
      "313/313 [==============================] - 0s 534us/step - loss: 0.4997 - accuracy: 0.7499\n",
      "Epoch 37/10000\n",
      "313/313 [==============================] - 0s 592us/step - loss: 0.5001 - accuracy: 0.7507\n",
      "Epoch 38/10000\n",
      "313/313 [==============================] - 0s 589us/step - loss: 0.4934 - accuracy: 0.7534\n",
      "Epoch 39/10000\n",
      "313/313 [==============================] - 0s 559us/step - loss: 0.4918 - accuracy: 0.7591\n",
      "Epoch 40/10000\n",
      "313/313 [==============================] - 0s 595us/step - loss: 0.4909 - accuracy: 0.7585\n",
      "Epoch 41/10000\n",
      "313/313 [==============================] - 0s 595us/step - loss: 0.4893 - accuracy: 0.7617\n",
      "Epoch 42/10000\n",
      "313/313 [==============================] - 0s 553us/step - loss: 0.4872 - accuracy: 0.7606\n",
      "Epoch 43/10000\n",
      "313/313 [==============================] - 0s 588us/step - loss: 0.4845 - accuracy: 0.7652\n",
      "Epoch 44/10000\n",
      "313/313 [==============================] - 0s 583us/step - loss: 0.4798 - accuracy: 0.7650\n",
      "Epoch 45/10000\n",
      "313/313 [==============================] - 0s 566us/step - loss: 0.4768 - accuracy: 0.7710\n",
      "Epoch 46/10000\n",
      "313/313 [==============================] - 0s 624us/step - loss: 0.4772 - accuracy: 0.7683\n",
      "Epoch 47/10000\n",
      "313/313 [==============================] - 0s 627us/step - loss: 0.4732 - accuracy: 0.7715\n",
      "Epoch 48/10000\n",
      "313/313 [==============================] - 0s 665us/step - loss: 0.4717 - accuracy: 0.7709\n",
      "Epoch 49/10000\n",
      "313/313 [==============================] - 0s 600us/step - loss: 0.4707 - accuracy: 0.7731\n",
      "Epoch 50/10000\n",
      "313/313 [==============================] - 0s 686us/step - loss: 0.4682 - accuracy: 0.7711\n",
      "Epoch 51/10000\n",
      "313/313 [==============================] - 0s 739us/step - loss: 0.4660 - accuracy: 0.7718\n",
      "Epoch 52/10000\n",
      "313/313 [==============================] - 0s 613us/step - loss: 0.4659 - accuracy: 0.7776\n",
      "Epoch 53/10000\n",
      "313/313 [==============================] - 0s 689us/step - loss: 0.4611 - accuracy: 0.7760\n",
      "Epoch 54/10000\n",
      "313/313 [==============================] - 0s 630us/step - loss: 0.4557 - accuracy: 0.7811\n",
      "Epoch 55/10000\n",
      "313/313 [==============================] - 0s 579us/step - loss: 0.4592 - accuracy: 0.7794\n",
      "Epoch 56/10000\n",
      "313/313 [==============================] - 0s 626us/step - loss: 0.4536 - accuracy: 0.7838\n",
      "Epoch 57/10000\n",
      "313/313 [==============================] - 0s 598us/step - loss: 0.4499 - accuracy: 0.7847\n",
      "Epoch 58/10000\n",
      "313/313 [==============================] - 0s 548us/step - loss: 0.4495 - accuracy: 0.7882\n",
      "Epoch 59/10000\n",
      "313/313 [==============================] - 0s 588us/step - loss: 0.4444 - accuracy: 0.7926\n",
      "Epoch 60/10000\n",
      "313/313 [==============================] - 0s 663us/step - loss: 0.4467 - accuracy: 0.7866\n",
      "Epoch 61/10000\n",
      "313/313 [==============================] - 0s 750us/step - loss: 0.4438 - accuracy: 0.7890\n",
      "Epoch 62/10000\n",
      "313/313 [==============================] - 0s 561us/step - loss: 0.4435 - accuracy: 0.7943\n",
      "Epoch 63/10000\n",
      "313/313 [==============================] - 0s 570us/step - loss: 0.4403 - accuracy: 0.7902\n",
      "Epoch 64/10000\n",
      "313/313 [==============================] - 0s 608us/step - loss: 0.4401 - accuracy: 0.7951\n",
      "Epoch 65/10000\n",
      "313/313 [==============================] - 0s 534us/step - loss: 0.4389 - accuracy: 0.7938\n",
      "Epoch 66/10000\n",
      "313/313 [==============================] - 0s 605us/step - loss: 0.4357 - accuracy: 0.7963\n",
      "Epoch 67/10000\n",
      "313/313 [==============================] - 0s 580us/step - loss: 0.4330 - accuracy: 0.7924\n",
      "Epoch 68/10000\n",
      "313/313 [==============================] - 0s 564us/step - loss: 0.4326 - accuracy: 0.7988\n",
      "Epoch 69/10000\n",
      "313/313 [==============================] - 0s 592us/step - loss: 0.4304 - accuracy: 0.7985\n",
      "Epoch 70/10000\n",
      "313/313 [==============================] - 0s 605us/step - loss: 0.4275 - accuracy: 0.7990\n",
      "Epoch 71/10000\n",
      "313/313 [==============================] - 0s 548us/step - loss: 0.4275 - accuracy: 0.7998\n",
      "Epoch 72/10000\n",
      "313/313 [==============================] - 0s 616us/step - loss: 0.4213 - accuracy: 0.8064\n",
      "Epoch 73/10000\n",
      "313/313 [==============================] - 0s 590us/step - loss: 0.4215 - accuracy: 0.8032\n",
      "Epoch 74/10000\n",
      "313/313 [==============================] - 0s 591us/step - loss: 0.4219 - accuracy: 0.8058\n",
      "Epoch 75/10000\n",
      "313/313 [==============================] - 0s 567us/step - loss: 0.4174 - accuracy: 0.8061\n",
      "Epoch 76/10000\n",
      "313/313 [==============================] - 0s 578us/step - loss: 0.4194 - accuracy: 0.8023\n",
      "Epoch 77/10000\n",
      "313/313 [==============================] - 0s 559us/step - loss: 0.4122 - accuracy: 0.8089\n",
      "Epoch 78/10000\n",
      "313/313 [==============================] - 0s 602us/step - loss: 0.4113 - accuracy: 0.8106\n",
      "Epoch 79/10000\n",
      "313/313 [==============================] - 0s 602us/step - loss: 0.4163 - accuracy: 0.8055\n",
      "Epoch 80/10000\n",
      "313/313 [==============================] - 0s 606us/step - loss: 0.4097 - accuracy: 0.8128\n",
      "Epoch 81/10000\n",
      "313/313 [==============================] - 0s 578us/step - loss: 0.4123 - accuracy: 0.8102\n",
      "Epoch 82/10000\n",
      "313/313 [==============================] - 0s 580us/step - loss: 0.4081 - accuracy: 0.8140\n",
      "Epoch 83/10000\n",
      "313/313 [==============================] - 0s 581us/step - loss: 0.4051 - accuracy: 0.8116\n",
      "Epoch 84/10000\n",
      "313/313 [==============================] - 0s 551us/step - loss: 0.4076 - accuracy: 0.8097\n",
      "Epoch 85/10000\n",
      "313/313 [==============================] - 0s 578us/step - loss: 0.4052 - accuracy: 0.8140\n",
      "Epoch 86/10000\n",
      "313/313 [==============================] - 0s 597us/step - loss: 0.4021 - accuracy: 0.8143\n",
      "Epoch 87/10000\n",
      "313/313 [==============================] - 0s 549us/step - loss: 0.4007 - accuracy: 0.8167\n",
      "Epoch 88/10000\n",
      "313/313 [==============================] - 0s 596us/step - loss: 0.4009 - accuracy: 0.8174\n",
      "Epoch 89/10000\n",
      "313/313 [==============================] - 0s 573us/step - loss: 0.3899 - accuracy: 0.8194\n",
      "Epoch 90/10000\n",
      "313/313 [==============================] - 0s 568us/step - loss: 0.3942 - accuracy: 0.8220\n",
      "Epoch 91/10000\n",
      "313/313 [==============================] - 0s 600us/step - loss: 0.3928 - accuracy: 0.8225\n",
      "Epoch 92/10000\n",
      "313/313 [==============================] - 0s 583us/step - loss: 0.3956 - accuracy: 0.8183\n",
      "Epoch 93/10000\n",
      "313/313 [==============================] - 0s 559us/step - loss: 0.3923 - accuracy: 0.8204\n",
      "Epoch 94/10000\n",
      "313/313 [==============================] - 0s 611us/step - loss: 0.3924 - accuracy: 0.8229\n",
      "Epoch 95/10000\n",
      "313/313 [==============================] - 0s 597us/step - loss: 0.3882 - accuracy: 0.8270\n",
      "Epoch 96/10000\n",
      "313/313 [==============================] - 0s 568us/step - loss: 0.3900 - accuracy: 0.8241\n",
      "Epoch 97/10000\n",
      "313/313 [==============================] - 0s 590us/step - loss: 0.3835 - accuracy: 0.8266\n",
      "Epoch 98/10000\n",
      "313/313 [==============================] - 0s 601us/step - loss: 0.3779 - accuracy: 0.8307\n",
      "Epoch 99/10000\n",
      "313/313 [==============================] - 0s 539us/step - loss: 0.3850 - accuracy: 0.8249\n",
      "Epoch 100/10000\n",
      "313/313 [==============================] - 0s 582us/step - loss: 0.3811 - accuracy: 0.8261\n",
      "Epoch 101/10000\n",
      "313/313 [==============================] - 0s 606us/step - loss: 0.3779 - accuracy: 0.8270\n",
      "Epoch 102/10000\n",
      "313/313 [==============================] - 0s 543us/step - loss: 0.3794 - accuracy: 0.8262\n",
      "Epoch 103/10000\n",
      "313/313 [==============================] - 0s 590us/step - loss: 0.3788 - accuracy: 0.8273\n",
      "Epoch 104/10000\n",
      "313/313 [==============================] - 0s 622us/step - loss: 0.3764 - accuracy: 0.8313\n",
      "Epoch 105/10000\n",
      "313/313 [==============================] - 0s 532us/step - loss: 0.3731 - accuracy: 0.8351\n",
      "Epoch 106/10000\n",
      "313/313 [==============================] - 0s 590us/step - loss: 0.3722 - accuracy: 0.8318\n",
      "Epoch 107/10000\n",
      "313/313 [==============================] - 0s 608us/step - loss: 0.3735 - accuracy: 0.8275\n",
      "Epoch 108/10000\n",
      "313/313 [==============================] - 0s 538us/step - loss: 0.3680 - accuracy: 0.8359\n",
      "Epoch 109/10000\n",
      "313/313 [==============================] - 0s 588us/step - loss: 0.3739 - accuracy: 0.8365\n",
      "Epoch 110/10000\n",
      "313/313 [==============================] - 0s 576us/step - loss: 0.3673 - accuracy: 0.8368\n",
      "Epoch 111/10000\n",
      "313/313 [==============================] - 0s 578us/step - loss: 0.3719 - accuracy: 0.8331\n",
      "Epoch 112/10000\n",
      "313/313 [==============================] - 0s 599us/step - loss: 0.3627 - accuracy: 0.8375\n",
      "Epoch 113/10000\n",
      "313/313 [==============================] - 0s 586us/step - loss: 0.3677 - accuracy: 0.8336\n",
      "Epoch 114/10000\n",
      "313/313 [==============================] - 0s 560us/step - loss: 0.3668 - accuracy: 0.8373\n",
      "Epoch 115/10000\n",
      "313/313 [==============================] - 0s 599us/step - loss: 0.3646 - accuracy: 0.8378\n",
      "Epoch 116/10000\n",
      "313/313 [==============================] - 0s 576us/step - loss: 0.3600 - accuracy: 0.8399\n",
      "Epoch 117/10000\n",
      "313/313 [==============================] - 0s 543us/step - loss: 0.3730 - accuracy: 0.8323\n",
      "Epoch 118/10000\n",
      "313/313 [==============================] - 0s 611us/step - loss: 0.3608 - accuracy: 0.8415\n",
      "Epoch 119/10000\n",
      "313/313 [==============================] - 0s 559us/step - loss: 0.3627 - accuracy: 0.8413\n",
      "Epoch 120/10000\n",
      "313/313 [==============================] - 0s 583us/step - loss: 0.3577 - accuracy: 0.8399\n",
      "Epoch 121/10000\n",
      "313/313 [==============================] - 0s 606us/step - loss: 0.3583 - accuracy: 0.8386\n",
      "Epoch 122/10000\n",
      "313/313 [==============================] - 0s 586us/step - loss: 0.3569 - accuracy: 0.8445\n",
      "Epoch 123/10000\n",
      "313/313 [==============================] - 0s 563us/step - loss: 0.3555 - accuracy: 0.8430\n",
      "Epoch 124/10000\n",
      "313/313 [==============================] - 0s 593us/step - loss: 0.3520 - accuracy: 0.8442\n",
      "Epoch 125/10000\n",
      "313/313 [==============================] - 0s 579us/step - loss: 0.3634 - accuracy: 0.8406\n",
      "Epoch 126/10000\n",
      "313/313 [==============================] - 0s 571us/step - loss: 0.3490 - accuracy: 0.8445\n",
      "Epoch 127/10000\n",
      "313/313 [==============================] - 0s 596us/step - loss: 0.3536 - accuracy: 0.8448\n",
      "Epoch 128/10000\n",
      "313/313 [==============================] - 0s 691us/step - loss: 0.3509 - accuracy: 0.8433\n",
      "Epoch 129/10000\n",
      "313/313 [==============================] - 0s 599us/step - loss: 0.3498 - accuracy: 0.8464\n",
      "Epoch 130/10000\n",
      "313/313 [==============================] - 0s 606us/step - loss: 0.3527 - accuracy: 0.8444\n",
      "Epoch 131/10000\n",
      "313/313 [==============================] - 0s 669us/step - loss: 0.3512 - accuracy: 0.8427\n",
      "Epoch 132/10000\n",
      "313/313 [==============================] - 0s 649us/step - loss: 0.3503 - accuracy: 0.8442\n",
      "Epoch 133/10000\n",
      "313/313 [==============================] - 0s 619us/step - loss: 0.3445 - accuracy: 0.8459\n",
      "Epoch 134/10000\n",
      "313/313 [==============================] - 0s 702us/step - loss: 0.3435 - accuracy: 0.8502\n",
      "Epoch 135/10000\n",
      "313/313 [==============================] - 0s 587us/step - loss: 0.3441 - accuracy: 0.8464\n",
      "Epoch 136/10000\n",
      "313/313 [==============================] - 0s 573us/step - loss: 0.3460 - accuracy: 0.8481\n",
      "Epoch 137/10000\n",
      "313/313 [==============================] - 0s 563us/step - loss: 0.3447 - accuracy: 0.8501\n",
      "Epoch 138/10000\n",
      "313/313 [==============================] - 0s 551us/step - loss: 0.3392 - accuracy: 0.8508\n",
      "Epoch 139/10000\n",
      "313/313 [==============================] - 0s 564us/step - loss: 0.3425 - accuracy: 0.8467\n",
      "Epoch 140/10000\n",
      "313/313 [==============================] - 0s 669us/step - loss: 0.3386 - accuracy: 0.8512\n",
      "Epoch 141/10000\n",
      "313/313 [==============================] - 0s 556us/step - loss: 0.3453 - accuracy: 0.8447\n",
      "Epoch 142/10000\n",
      "313/313 [==============================] - 0s 647us/step - loss: 0.3383 - accuracy: 0.8501\n",
      "Epoch 143/10000\n",
      "313/313 [==============================] - 0s 667us/step - loss: 0.3369 - accuracy: 0.8513\n",
      "Epoch 144/10000\n",
      "313/313 [==============================] - 0s 568us/step - loss: 0.3323 - accuracy: 0.8552\n",
      "Epoch 145/10000\n",
      "313/313 [==============================] - 0s 603us/step - loss: 0.3332 - accuracy: 0.8576\n",
      "Epoch 146/10000\n",
      "313/313 [==============================] - 0s 654us/step - loss: 0.3353 - accuracy: 0.8518\n",
      "Epoch 147/10000\n",
      "313/313 [==============================] - 0s 619us/step - loss: 0.3348 - accuracy: 0.8529\n",
      "Epoch 148/10000\n",
      "313/313 [==============================] - 0s 594us/step - loss: 0.3338 - accuracy: 0.8520\n",
      "Epoch 149/10000\n",
      "313/313 [==============================] - 0s 629us/step - loss: 0.3365 - accuracy: 0.8526\n",
      "Epoch 150/10000\n",
      "313/313 [==============================] - 0s 591us/step - loss: 0.3366 - accuracy: 0.8526\n",
      "Epoch 151/10000\n",
      "313/313 [==============================] - 0s 577us/step - loss: 0.3288 - accuracy: 0.8550\n",
      "Epoch 152/10000\n",
      "313/313 [==============================] - 0s 658us/step - loss: 0.3328 - accuracy: 0.8567\n",
      "Epoch 153/10000\n",
      "313/313 [==============================] - 0s 869us/step - loss: 0.3273 - accuracy: 0.8600\n",
      "Epoch 154/10000\n",
      "313/313 [==============================] - 0s 718us/step - loss: 0.3247 - accuracy: 0.8599\n",
      "Epoch 155/10000\n",
      "313/313 [==============================] - 0s 574us/step - loss: 0.3267 - accuracy: 0.8594\n",
      "Epoch 156/10000\n",
      "313/313 [==============================] - 0s 650us/step - loss: 0.3275 - accuracy: 0.8573\n",
      "Epoch 157/10000\n",
      "313/313 [==============================] - 0s 636us/step - loss: 0.3228 - accuracy: 0.8557\n",
      "Epoch 158/10000\n",
      "313/313 [==============================] - 0s 577us/step - loss: 0.3246 - accuracy: 0.8584\n",
      "Epoch 159/10000\n",
      "313/313 [==============================] - 0s 679us/step - loss: 0.3272 - accuracy: 0.8629\n",
      "Epoch 160/10000\n",
      "313/313 [==============================] - 0s 592us/step - loss: 0.3250 - accuracy: 0.8585\n",
      "Epoch 161/10000\n",
      "313/313 [==============================] - 0s 531us/step - loss: 0.3241 - accuracy: 0.8594\n",
      "Epoch 162/10000\n",
      "313/313 [==============================] - 0s 625us/step - loss: 0.3190 - accuracy: 0.8628\n",
      "Epoch 163/10000\n",
      "313/313 [==============================] - 0s 643us/step - loss: 0.3200 - accuracy: 0.8605\n",
      "Epoch 164/10000\n",
      "313/313 [==============================] - 0s 663us/step - loss: 0.3213 - accuracy: 0.8614\n",
      "Epoch 165/10000\n",
      "313/313 [==============================] - 0s 636us/step - loss: 0.3196 - accuracy: 0.8597\n",
      "Epoch 166/10000\n",
      "313/313 [==============================] - 0s 586us/step - loss: 0.3225 - accuracy: 0.8591\n",
      "Epoch 167/10000\n",
      "313/313 [==============================] - 0s 592us/step - loss: 0.3140 - accuracy: 0.8663\n",
      "Epoch 168/10000\n",
      "313/313 [==============================] - 0s 543us/step - loss: 0.3178 - accuracy: 0.8641\n",
      "Epoch 169/10000\n",
      "313/313 [==============================] - 0s 592us/step - loss: 0.3118 - accuracy: 0.8667\n",
      "Epoch 170/10000\n",
      "313/313 [==============================] - 0s 560us/step - loss: 0.3241 - accuracy: 0.8627\n",
      "Epoch 171/10000\n",
      "313/313 [==============================] - 0s 601us/step - loss: 0.3110 - accuracy: 0.8671\n",
      "Epoch 172/10000\n",
      "313/313 [==============================] - 0s 613us/step - loss: 0.3089 - accuracy: 0.8690\n",
      "Epoch 173/10000\n",
      "313/313 [==============================] - 0s 581us/step - loss: 0.3114 - accuracy: 0.8666\n",
      "Epoch 174/10000\n",
      "313/313 [==============================] - 0s 546us/step - loss: 0.3135 - accuracy: 0.8679\n",
      "Epoch 175/10000\n",
      "313/313 [==============================] - 0s 628us/step - loss: 0.3081 - accuracy: 0.8692\n",
      "Epoch 176/10000\n",
      "313/313 [==============================] - 0s 602us/step - loss: 0.3084 - accuracy: 0.8695\n",
      "Epoch 177/10000\n",
      "313/313 [==============================] - 0s 597us/step - loss: 0.3121 - accuracy: 0.8686\n",
      "Epoch 178/10000\n",
      "313/313 [==============================] - 0s 576us/step - loss: 0.3060 - accuracy: 0.8702\n",
      "Epoch 179/10000\n",
      "313/313 [==============================] - 0s 592us/step - loss: 0.3055 - accuracy: 0.8729\n",
      "Epoch 180/10000\n",
      "313/313 [==============================] - 0s 597us/step - loss: 0.3103 - accuracy: 0.8674\n",
      "Epoch 181/10000\n",
      "313/313 [==============================] - 0s 560us/step - loss: 0.3101 - accuracy: 0.8675\n",
      "Epoch 182/10000\n",
      "313/313 [==============================] - 0s 578us/step - loss: 0.3053 - accuracy: 0.8695\n",
      "Epoch 183/10000\n",
      "313/313 [==============================] - 0s 600us/step - loss: 0.3098 - accuracy: 0.8668\n",
      "Epoch 184/10000\n",
      "313/313 [==============================] - 0s 542us/step - loss: 0.3057 - accuracy: 0.8718\n",
      "Epoch 185/10000\n",
      "313/313 [==============================] - 0s 594us/step - loss: 0.3048 - accuracy: 0.8692\n",
      "Epoch 186/10000\n",
      "313/313 [==============================] - 0s 599us/step - loss: 0.3031 - accuracy: 0.8739\n",
      "Epoch 187/10000\n",
      "313/313 [==============================] - 0s 537us/step - loss: 0.2987 - accuracy: 0.8724\n",
      "Epoch 188/10000\n",
      "313/313 [==============================] - 0s 595us/step - loss: 0.3043 - accuracy: 0.8725\n",
      "Epoch 189/10000\n",
      "313/313 [==============================] - 0s 576us/step - loss: 0.2969 - accuracy: 0.8746\n",
      "Epoch 190/10000\n",
      "313/313 [==============================] - 0s 569us/step - loss: 0.2987 - accuracy: 0.8757\n",
      "Epoch 191/10000\n",
      "313/313 [==============================] - 0s 595us/step - loss: 0.2961 - accuracy: 0.8781\n",
      "Epoch 192/10000\n",
      "313/313 [==============================] - 0s 578us/step - loss: 0.3015 - accuracy: 0.8697\n",
      "Epoch 193/10000\n",
      "313/313 [==============================] - 0s 566us/step - loss: 0.2896 - accuracy: 0.8798\n",
      "Epoch 194/10000\n",
      "313/313 [==============================] - 0s 604us/step - loss: 0.2977 - accuracy: 0.8748\n",
      "Epoch 195/10000\n",
      "313/313 [==============================] - 0s 588us/step - loss: 0.2997 - accuracy: 0.8725\n",
      "Epoch 196/10000\n",
      "313/313 [==============================] - 0s 548us/step - loss: 0.2969 - accuracy: 0.8755\n",
      "Epoch 197/10000\n",
      "313/313 [==============================] - 0s 579us/step - loss: 0.2914 - accuracy: 0.8766\n",
      "Epoch 198/10000\n",
      "313/313 [==============================] - 0s 562us/step - loss: 0.2984 - accuracy: 0.8753\n",
      "Epoch 199/10000\n",
      "313/313 [==============================] - 0s 639us/step - loss: 0.2944 - accuracy: 0.8764\n",
      "Epoch 200/10000\n",
      "313/313 [==============================] - 0s 595us/step - loss: 0.2927 - accuracy: 0.8767\n",
      "Epoch 201/10000\n",
      "313/313 [==============================] - 0s 586us/step - loss: 0.2910 - accuracy: 0.8754\n",
      "Epoch 202/10000\n",
      "313/313 [==============================] - 0s 558us/step - loss: 0.2975 - accuracy: 0.8732\n",
      "Epoch 203/10000\n",
      "313/313 [==============================] - 0s 611us/step - loss: 0.2910 - accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "def mlp_model(neuronios, camadas, dropout, learning_rate, ativacao, otimizador):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuronios,input_dim=243, activation=ativacao))\n",
    "    model.add(Dropout(dropout))\n",
    "    if camadas==2:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    if otimizador =='adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif otimizador =='SGD':\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif otimizador =='RMSprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif otimizador =='Adadelta':\n",
    "        opt = keras.optimizers.Adadelta(learning_rate=learning_rate)\n",
    "        \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer=opt)\n",
    "    return model\n",
    "\n",
    "n_neuronios = [8, 16, 32, 64, 128, 512, 1024]\n",
    "n_camadas = [1, 2]\n",
    "n_dropout = [0.4, 0.2, 0.1]\n",
    "n_learning_rate = [0.1, 0.01, 0.001]\n",
    "n_ativacao = ['tanh', 'relu']\n",
    "n_otimizador = ['adam', 'SGD', 'RMSprop','Adadelta']\n",
    "\n",
    "# ds_2 = ds\n",
    "ds_2 = ds.iloc[ 0:10000, : ]\n",
    "\n",
    "trY = ds_2['IND_BOM_1_2']\n",
    "trX = ds_2.drop('IND_BOM_1_2', axis=1)\n",
    "trX = trX.drop('IND_BOM_1_1', axis=1)\n",
    "# trX = trX.drop('INDEX', axis=1)\n",
    "\n",
    "\n",
    "model = mlp_model(32, 2, 0.1, 0.0005,'relu','adam')\n",
    "\n",
    "paradinha = EarlyStopping(monitor='accuracy', mode='max', patience=10)\n",
    "history = model.fit(trX, trY, epochs=10000, verbose=1, callbacks=[paradinha])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 1s 626us/step - loss: 5.3330 - accuracy: 0.6543\n"
     ]
    }
   ],
   "source": [
    "ds_3 = ds.iloc[ 50001:100001, :]\n",
    "trY = ds_3['IND_BOM_1_2']\n",
    "trX = ds_3.drop('IND_BOM_1_2', axis=1)\n",
    "x, y = model.evaluate(trX, trY, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.635703645629165\n",
      "Recall: 0.56890631125049\n",
      "Precision: 0.6566283594244865\n",
      "F1: 0.6096278249180879\n",
      "AUROC: 0.6877607452577378\n",
      "AURP: 0.6860913885968859\n"
     ]
    }
   ],
   "source": [
    "# Cuidado: usar esses parâmetros muito elevados, ou o default (100 estimadores e profundidade ilimitada) vai travar seu computador\n",
    "\n",
    "# Parâmetros default:\n",
    "# n_estimators=100, *,\n",
    "# criterion=\"gini\",\n",
    "# max_depth=None,\n",
    "# min_samples_split=2,\n",
    "# min_samples_leaf=1,\n",
    "# min_weight_fraction_leaf=0.,\n",
    "# max_features=\"auto\",\n",
    "# max_leaf_nodes=None,\n",
    "# min_impurity_decrease=0.,\n",
    "# min_impurity_split=None,\n",
    "# bootstrap=True,\n",
    "# oob_score=False,\n",
    "# n_jobs=None,\n",
    "# random_state=None,\n",
    "# verbose=0,\n",
    "# warm_start=False,\n",
    "# class_weight=None,\n",
    "# ccp_alpha=0.0,\n",
    "# max_samples=None\n",
    "\n",
    "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
    "    accuracy = accuracy_score(y, y_pred_class)\n",
    "    recall = recall_score(y, y_pred_class)\n",
    "    precision = precision_score(y, y_pred_class)\n",
    "    f1 = f1_score(y, y_pred_class)\n",
    "    performance_metrics = (accuracy, recall, precision, f1)\n",
    "    if y_pred_scores is not None:\n",
    "        skplt.metrics.plot_ks_statistic(y, y_pred_scores)\n",
    "        plt.show()\n",
    "        y_pred_scores = y_pred_scores[:, 1]\n",
    "        auroc = roc_auc_score(y, y_pred_scores)\n",
    "        aupr = average_precision_score(y, y_pred_scores)\n",
    "        performance_metrics = performance_metrics + (auroc, aupr)\n",
    "    return performance_metrics\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=128, max_depth=8, min_samples_leaf=2, min_samples_split=4, oob_score=True)\n",
    "df_treino = treino_ina.drop(['IND_BOM_1_2','IND_BOM_1_1'], axis=1).append(treino_adi.drop(['IND_BOM_1_2','IND_BOM_1_1'], axis=1))\n",
    "df_validacao = valid_ina.drop(['IND_BOM_1_2','IND_BOM_1_1'], axis=1).append(valid_adi.drop(['IND_BOM_1_2','IND_BOM_1_1'], axis=1))\n",
    "df_validacao.drop(columns=['INDEX'], axis=1, inplace=True)\n",
    "\n",
    "random_forest.fit(df_treino, [0]*len(treino_ina)+[1]*len(treino_adi))\n",
    "\n",
    "rf_pred_class  = random_forest.predict(df_validacao)\n",
    "rf_pred_scores = random_forest.predict_proba(df_validacao)\n",
    "\n",
    "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics([0]*len(valid_ina)+[1]*len(valid_adi), rf_pred_class, rf_pred_scores)\n",
    "print('Acurácia: {}\\nRecall: {}\\nPrecision: {}\\nF1: {}\\nAUROC: {}\\nAURP: {}'.format(accuracy, recall, precision, f1, auroc, aupr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-25 19:01:19,179]\u001b[0m A new study created in memory with name: no-name-287b067a-349f-48dd-9bc9-60f1dacd8722\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest com 7 estimadores e profundidade máxima 5, critério entropy, min amostras de folhas 7 e min divisão de amostras 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-25 19:01:23,653]\u001b[0m Trial 0 finished with value: 0.6158526068208545 and parameters: {'n_estimators': 7, 'max_depth': 5, 'criterion': 'entropy', 'min_samples_leaf': 7, 'min_samples_split': 2}. Best is trial 0 with value: 0.6158526068208545.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.6158526068208545\n",
      "Recall: 0.5204233633869071\n",
      "Precision: 0.6431796600972811\n",
      "F1: 0.5753263187089394\n",
      "AUROC: 0.6617996993976736\n",
      "AURP: 0.6598541224326315\n",
      "Random Forest com 5 estimadores e profundidade máxima 2, critério gini, min amostras de folhas 3 e min divisão de amostras 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-25 19:01:25,331]\u001b[0m Trial 1 finished with value: 0.5636456291650334 and parameters: {'n_estimators': 5, 'max_depth': 2, 'criterion': 'gini', 'min_samples_leaf': 3, 'min_samples_split': 7}. Best is trial 0 with value: 0.6158526068208545.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.5636456291650334\n",
      "Recall: 0.4412230497843983\n",
      "Precision: 0.584281561461794\n",
      "F1: 0.5027739312994148\n",
      "AUROC: 0.6043091812174717\n",
      "AURP: 0.5969463864885365\n",
      "Random Forest com 6 estimadores e profundidade máxima 10, critério gini, min amostras de folhas 6 e min divisão de amostras 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-25 19:01:31,484]\u001b[0m Trial 2 finished with value: 0.626844374754998 and parameters: {'n_estimators': 6, 'max_depth': 10, 'criterion': 'gini', 'min_samples_leaf': 6, 'min_samples_split': 3}. Best is trial 2 with value: 0.626844374754998.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.626844374754998\n",
      "Recall: 0.5760564484515877\n",
      "Precision: 0.6411854023770878\n",
      "F1: 0.6068785516056561\n",
      "AUROC: 0.6760568607694294\n",
      "AURP: 0.673766642815074\n",
      "Random Forest com 8 estimadores e profundidade máxima 5, critério gini, min amostras de folhas 3 e min divisão de amostras 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-25 19:01:35,867]\u001b[0m Trial 3 finished with value: 0.6166601332810663 and parameters: {'n_estimators': 8, 'max_depth': 5, 'criterion': 'gini', 'min_samples_leaf': 3, 'min_samples_split': 8}. Best is trial 2 with value: 0.626844374754998.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.6166601332810663\n",
      "Recall: 0.5284986279890239\n",
      "Precision: 0.6416333523700742\n",
      "F1: 0.5795967499247667\n",
      "AUROC: 0.666919171104682\n",
      "AURP: 0.6645444549549462\n",
      "Random Forest com 5 estimadores e profundidade máxima 12, critério entropy, min amostras de folhas 3 e min divisão de amostras 4\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1c041bcbca53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mstudy_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mstudy_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \"\"\"\n\u001b[0;32m--> 385\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-1c041bcbca53>\u001b[0m in \u001b[0;36mrf\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_treino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreino_ina\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreino_adi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mrf_pred_class\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_validacao\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mrf_pred_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_validacao\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_performance_metrics_sem_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ina\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_adi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_pred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_pred_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Acurácia: {}\\nRecall: {}\\nPrecision: {}\\nF1: {}\\nAUROC: {}\\nAURP: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    681\u001b[0m                      for j in np.atleast_1d(self.n_classes_)]\n\u001b[1;32m    682\u001b[0m         \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         Parallel(n_jobs=n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    684\u001b[0m                  \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sharedmem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0mcomplains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mplaced\u001b[0m \u001b[0mthere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "ds_dropado = ds.drop(labels=['IND_BOM_1_2', 'IND_BOM_1_1'], axis=1)\n",
    "df_treino = treino_ina.drop(['IND_BOM_1_2','IND_BOM_1_1'], axis=1).append(treino_adi.drop(['IND_BOM_1_2','IND_BOM_1_1'], axis=1))\n",
    "df_validacao = valid_ina.drop(['IND_BOM_1_2','IND_BOM_1_1'], axis=1).append(valid_adi.drop(['IND_BOM_1_2','IND_BOM_1_1'], axis=1))\n",
    "df_validacao.drop(columns=['INDEX'], axis=1, inplace=True)\n",
    "\n",
    "def compute_performance_metrics_sem_plot(y, y_pred_class, y_pred_scores=None):\n",
    "    accuracy = accuracy_score(y, y_pred_class)\n",
    "    recall = recall_score(y, y_pred_class)\n",
    "    precision = precision_score(y, y_pred_class)\n",
    "    f1 = f1_score(y, y_pred_class)\n",
    "    performance_metrics = (accuracy, recall, precision, f1)\n",
    "    if y_pred_scores is not None:\n",
    "        # skplt.metrics.plot_ks_statistic(y, y_pred_scores)\n",
    "        # plt.show()\n",
    "        y_pred_scores = y_pred_scores[:, 1]\n",
    "        auroc = roc_auc_score(y, y_pred_scores)\n",
    "        aupr = average_precision_score(y, y_pred_scores)\n",
    "        performance_metrics = performance_metrics + (auroc, aupr)\n",
    "    return performance_metrics\n",
    "\n",
    "def rf(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 16, 128)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 16)\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 8)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 8)\n",
    "    random_forest = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion=criterion, bootstrap=True, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split)\n",
    "    print('Random Forest com {} estimadores e profundidade máxima {}, critério {}, min amostras de folhas {} e min divisão de amostras {}\\n'.format(\n",
    "        n_estimators, max_depth, criterion, min_samples_leaf, min_samples_split\n",
    "    ))\n",
    "    # start = time.time()\n",
    "    # n_scores = cross_val_score(random_forest, ds_dropado, ds['IND_BOM_1_2'], scoring='accuracy', n_jobs=1, error_score='raise')\n",
    "    # print('Acurácia e desvio padrão: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "    # print(\"Tempo: \", format(time.time() - start, '.3f'), 's\\n', sep='')\n",
    "\n",
    "    random_forest.fit(df_treino, [0]*len(treino_ina)+[1]*len(treino_adi))\n",
    "    rf_pred_class  = random_forest.predict(df_validacao)\n",
    "    rf_pred_scores = random_forest.predict_proba(df_validacao)\n",
    "    accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics_sem_plot([0]*len(valid_ina)+[1]*len(valid_adi), rf_pred_class, rf_pred_scores)\n",
    "    print('Acurácia: {}\\nRecall: {}\\nPrecision: {}\\nF1: {}\\nAUROC: {}\\nAURP: {}\\n\\n'.format(accuracy, recall, precision, f1, auroc, aupr))\n",
    "    return accuracy\n",
    "\n",
    "study_1 = optuna.create_study(direction=\"maximize\")\n",
    "study_1.optimize(rf, n_trials=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros default:\n",
    "# *,loss='deviance', \n",
    "# learning_rate=0.1, \n",
    "# n_estimators=100,\n",
    "# subsample=1.0, \n",
    "# criterion='friedman_mse', \n",
    "# min_samples_split=2,\n",
    "# min_samples_leaf=1, \n",
    "# min_weight_fraction_leaf=0.,\n",
    "# max_depth=3, \n",
    "# min_impurity_decrease=0.,\n",
    "# min_impurity_split=None, \n",
    "# random_state=None, \n",
    "# max_features=None, verbose=0,\n",
    "# max_leaf_nodes=None, \n",
    "# warm_start=False,\n",
    "# validation_fraction=0.1, \n",
    "# n_iter_no_change=None, \n",
    "# tol=1e-4,\n",
    "# ccp_alpha=0.0\n",
    "\n",
    "gradient_boost = GradientBoostingClassifier(n_estimators = 5, max_depth = 8)\n",
    "n_scores = cross_val_score(gradient_boost, x_rf, y_rf, scoring='accuracy', n_jobs=1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
